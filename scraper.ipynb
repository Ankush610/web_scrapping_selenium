{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beautiful Soup Scrapper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO WORK WITH BEAUTIFUL SOUP : \n",
    "    # pip install requests\n",
    "    # pip install html5lib\n",
    "    # pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL = \"https://en.wikipedia.org/wiki/Grand_Theft_Auto_VI\"\n",
    "# r = requests.get(URL)\n",
    "# r.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Code ( Selenium Scrapper on NSE data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import numpy as np\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one if we don't want a popup browser \n",
    "# options = webdriver.ChromeOptions()\n",
    "# options.add_argument(\"--headless=new\")\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get(\"https://www.nseindia.com/market-data/live-equity-market\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(driver.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = driver.find_elements(By.CLASS_NAME, 'symbol-word-break')\n",
    "values = driver.find_elements(By.CLASS_NAME, 'text-right')\n",
    "\n",
    "\n",
    "# Extracting the text from each WebElement\n",
    "stock_values = [stock.text for stock in stocks]\n",
    "values_values = [values.text for values in values]\n",
    "\n",
    "# Printing the values\n",
    "print(stock_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_values# Assuming `values_values` contains your data\n",
    "\n",
    "columns = 13  # Number of columns in each row\n",
    "\n",
    "# Reshape the list into sublists of 13 elements each\n",
    "table = [values_values[i:i + columns] for i in range(0, len(values_values), columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame(stock_values),pd.DataFrame(table[2:])],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Web Scraping Using Selenium**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Do you want to master **web scraping with Selenium** in bash, from the basics to advanced techniques, including strategies to avoid being blocked? Look no further! \n",
    "\n",
    "Selenium is a widely-used **open-source library** designed for **browser automation** and **scraping dynamic content**. It leverages the **WebDriver protocol** to control popular browsers like **Chrome**, **Firefox**, and **Safari**. \n",
    "\n",
    "Unlike traditional scraping tools, Selenium excels at interacting with **JavaScript-heavy websites**, giving it a significant edge. Its human-like interaction capabilities also make it effective at mimicking real users and bypassing anti-bot systems.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#2E86C1\"><b>Why Selenium?</b></span>\n",
    "\n",
    "- **Handles Dynamic Content**: Collects data from sites that render content using JavaScript.\n",
    "- **Human-Like Interaction**: Simulates real user actions like typing, scrolling, and clicking.\n",
    "- **Browser Compatibility**: Works with major browsers like Chrome, Firefox, Safari, and Edge.\n",
    "- **Powerful Automation**: Ideal for both web scraping and automated testing.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#D35400\"><b>Getting Started with Selenium</b></span>\n",
    "\n",
    "To scrape data using Selenium, we’ll use **Google Chrome**, one of the most popular browsers for automation. You’ll need to install the following tools:\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>Step 1: Install Google Chrome</b></span>\n",
    "\n",
    "Download the latest version of **Google Chrome** from the official website:\n",
    "[Download Google Chrome](https://www.google.it/intl/en/chrome/)\n",
    "\n",
    "### <span style=\"color:#E74C3C\"><b>Step 2: Install ChromeDriver</b></span>\n",
    "\n",
    "- Visit the official [ChromeDriver page](https://googlechromelabs.github.io/chrome-for-testing/).\n",
    "- Download the **ChromeDriver** version that matches your Chrome browser version.\n",
    "- Extract the zipped folder and locate the `chromedriver` executable.\n",
    "- Move the `chromedriver` file to your project root folder for easy access.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#F39C12\"><b>Step#1: Setting Up Selenium in bash</b></span>\n",
    "\n",
    "Use the following code snippet to set up Selenium for **Google Chrome**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install selenium  # install it in your current env "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required library\n",
    "from selenium import webdriver\n",
    " \n",
    "# initialize an instance of the chrome driver (browser)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# visit your target site\n",
    "driver.get(\"https://www.scrapingcourse.com/ecommerce/\")\n",
    "\n",
    "# output the full-page HTML\n",
    "# print(driver.page_source)\n",
    "\n",
    "# release the resources allocated by Selenium and shut down the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code spins up a browser interface with a \"Chrome is being controlled by automated test software\" message, an extra alert section to inform you that Selenium is controlling the Chrome instance:\n",
    "\n",
    "<center><img src=\"./images/SeleniumControlChrome.png\" alt=\"error\" width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Headless Browsers in bash**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A *headless browser* is a browser that operates without a **Graphical User Interface (GUI)** but retains all the functionalities of a regular browser. These browsers are controlled using automation scripts and are widely used for tasks such as **test automation** and **web scraping**.\n",
    "\n",
    "Headless browsers are faster than GUI-based browsers because they avoid rendering resource-intensive graphics. They allow you to execute JavaScript, automate interactions like **clicking**, **scrolling**, and **typing**, and handle dynamic websites efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#2E86C1\"><b>What Is a Headless Browser in bash?</b></span>\n",
    "\n",
    "A **bash headless browser** is an automation tool designed to perform browser operations invisibly, controlled through scripts. Popular headless browser tools in bash include **Selenium** and **Playwright**.\n",
    "\n",
    "### **Key Features:**\n",
    "- **No GUI**: Operates without graphical rendering, improving performance.\n",
    "- **Automation Capabilities**: Automate interactions like typing, clicking, scrolling, and more.\n",
    "- **Dynamic Content Handling**: Extract data from JavaScript-rendered web pages.\n",
    "- **Waiting Mechanism**: Adds delay for web elements to load before taking further actions.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#D35400\"><b>Step #2: Setting Up Headless Mode in Selenium</b></span>\n",
    "\n",
    "To enable *headless mode* in Selenium for Chrome, you use the `ChromeOptions` object. Here’s how to set it up:\n",
    "\n",
    "```bash\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Configure Chrome for headless mode\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless=new\")  # Enable headless mode for Chrome 109+\n",
    "chrome_options.add_argument(\"--disable-gpu\")   # Disable GPU for stability\n",
    "chrome_options.add_argument(\"--window-size=1920,1080\")  # Set browser window size\n",
    "\n",
    "# Start WebDriver with options\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "driver.get(\"https://example.com\")\n",
    "print(driver.title)\n",
    "driver.quit()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#28B463\"><b>Comparison of Popular bash Headless Browsers</b></span>\n",
    "\n",
    "| **Name**           | **Pros**                                                                                                                                                     | **Cons**                                                                                                                                                     |\n",
    "|---------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Selenium**        | - **Wide Browser Support**: Works with major browsers like Chrome, Firefox, Edge, Safari, and Internet Explorer.<br>- **Extensive Plugins**: Supports plugins like the Undetected ChromeDriver for bypassing bot detection.<br>- **Rich Ecosystem**: Well-documented and widely used in testing and scraping communities. | - **Complex for Advanced Tasks**: Requires a steep learning curve, especially for intricate automation or scraping tasks.<br>- **Limited Browser Control**: Provides limited direct manipulation of browser properties like the `navigator` field.<br>- **Messy JavaScript Execution**: Requires wrapping JavaScript code in strings, making it less intuitive compared to alternatives. |\n",
    "| **Playwright**      | - **Simple API Methods**: Offers straightforward and developer-friendly APIs for automation tasks.<br>- **Broad Browser Support**: Works with major browsers, including Chrome, Firefox, Safari, and Edge.<br>- **Fine-Grained Browser Control**: Allows direct manipulation of browser properties via the Chrome DevTools Protocol.<br>- **Anti-Bot Features**: Supports evasion plugins like Playwright Stealth for bypassing detection mechanisms. | - **Large Installation Size**: Downloading browser binaries increases disk space usage.<br>- **Limited Legacy Support**: Does not support older browsers like Internet Explorer, limiting its use for certain projects. |\n",
    "\n",
    "## <span style=\"color:#F39C12\"><b>Other Simple Tools</b></span>\n",
    "\n",
    "While Selenium and Playwright are the most feature-rich, other tools like **Pyppeteer**, **Splash**, and **MechanicalSoup** can also be used for simpler or specific tasks.\n",
    "\n",
    "- **Pyppeteer**: Good for Chromium-specific tasks but lacks updates.\n",
    "- **Splash**: Lightweight and integrates with Scrapy but requires Lua scripting.\n",
    "- **MechanicalSoup**: Ideal for static websites but limited for JavaScript-heavy pages.\n",
    "\n",
    "\n",
    "Choose a tool based on your project requirements, balancing simplicity, performance, and compatibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required library\n",
    "from selenium import webdriver\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "options.add_argument(\"--headless=new\")\n",
    " \n",
    "# initialize an instance of the chrome driver (browser)\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# visit your target site\n",
    "driver.get(\"https://www.scrapingcourse.com/ecommerce/\")\n",
    "\n",
    "# output the full-page HTML\n",
    "# print(driver.page_source)\n",
    "\n",
    "# release the resources allocated by Selenium and shut down the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step #3: Extract Specific Data From the Page**\n",
    "\n",
    "To extract specific data from a website, Selenium allows you to scrape information such as product names, prices, image sources, and URLs from the target page. \n",
    "\n",
    "Selenium provides two primary methods to locate elements on a web page:\n",
    "\n",
    "1. **`find_element`**: Retrieves a single element. If multiple elements match the selector, it returns the **first matching element**.\n",
    "2. **`find_elements`**: Retrieves **all elements** matching the selector as an array.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#28B463\"><b>Methods for Locating Elements</b></span>\n",
    "\n",
    "### **Categories of Locators**\n",
    "Selenium supports eight locator strategies divided into three main categories:\n",
    "\n",
    "- **CSS Selectors**: `By.ID`, `By.CLASS_NAME`, `By.CSS_SELECTOR`\n",
    "- **XPath**: `By.XPATH`\n",
    "- **Direct Selectors**: `By.NAME`, `By.LINK_TEXT`, `By.PARTIAL_LINK_TEXT`, `By.TAG_NAME`\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#2E86C1\"><b>Locator Strategies with Examples</b></span>\n",
    "\n",
    "The table below describes the strategies, their usage, and corresponding Selenium examples:\n",
    "\n",
    "| **Strategy**          | **Description**                                                | **HTML Sample Code**                      | **Selenium Example**                                                                 |\n",
    "|------------------------|---------------------------------------------------------------|-------------------------------------------|-------------------------------------------------------------------------------------|\n",
    "| **By.ID**             | Selects elements based on their `id` attribute                | `<div id=\"s-437\">...</div>`               | `find_element(By.ID, \"s-437\")`                                                     |\n",
    "| **By.CLASS_NAME**     | Selects elements based on their `class` attribute             | `<div class=\"welcome-text\">Welcome!</div>`| `find_element(By.CLASS_NAME, \"welcome-text\")` <br>`find_elements(By.CLASS_NAME, \"text-center\")` |\n",
    "| **By.CSS_SELECTOR**   | Selects elements matching a CSS selector                      | `<div class=\"product-card\"><span class=\"price\">$140</span></div>` | `find_element(By.CSS_SELECTOR, \".product-card .price\")` <br>`find_elements(By.CSS_SELECTOR, \".product-card .price\")` |\n",
    "| **By.XPATH**          | Selects elements using an XPath expression                   | `<h1>My <strong>Fantastic</strong> Blog</h1>` | `find_element(By.XPATH, \"//h1/strong\")` <br>`find_elements(By.XPATH, \"//h1/strong\")` |\n",
    "| **By.NAME**           | Selects elements based on their `name` attribute             | `<input name=\"email\" />`                  | `find_element(By.NAME, \"email\")` <br>`find_elements(By.NAME, \"email\")`             |\n",
    "| **By.LINK_TEXT**      | Selects anchor (`<a>`) elements matching a specific link text| `<a href=\"/\">Home</a>`                    | `find_element(By.LINK_TEXT, \"Home\")` <br>`find_elements(By.LINK_TEXT, \"Home\")`     |\n",
    "| **By.PARTIAL_LINK_TEXT** | Selects anchor (`<a>`) elements matching a substring of the link text | `<a href=\"/\">Click here now</a>` | `find_element(By.PARTIAL_LINK_TEXT, \"now\")` <br>`find_elements(By.PARTIAL_LINK_TEXT, \"now\")` |\n",
    "| **By.TAG_NAME**       | Selects elements based on their tag name                     | `<span>...</span>`                        | `find_element(By.TAG_NAME, \"span\")` <br>`find_elements(By.TAG_NAME, \"span\")`       |\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#F39C12\"><b>CSS Selectors vs XPath</b></span>\n",
    "\n",
    "- **CSS Selectors**: Recommended for beginners due to simplicity and maintainability. They are ideal for selecting elements with classes and IDs.\n",
    "- **XPath**: Useful for navigating complex HTML structures. It provides more specificity when selecting nodes.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#9B59B6\"><b>Quick Tips</b></span>\n",
    "\n",
    "1. **Get CSS Selectors and XPath Automatically**:  \n",
    "   Right-click on an element in the browser, open the **\"Copy\" menu**, and select either:\n",
    "   - **Copy selector** (for CSS Selector)\n",
    "   - **Copy XPath** (for XPath)\n",
    "\n",
    "2. **Choosing the Right Method**:  \n",
    "   Use **CSS Selectors** for simplicity. Use **XPath** for complex cases where CSS Selectors might not suffice.\n",
    "\n",
    "---\n",
    "\n",
    "Now that you understand the element locators and their applications, try them out to scrape data effectively!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, import By, the Selenium method containing all the built-in locator strategies\n",
    "\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required library\n",
    "from selenium import webdriver\n",
    "\n",
    "# options = webdriver.ChromeOptions()\n",
    "\n",
    "# options.add_argument(\"--headless=new\")\n",
    " \n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get(\"https://www.scrapingcourse.com/ecommerce/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all the product containers\n",
    "products = driver.find_elements(By.CSS_SELECTOR, \".product\")\n",
    "products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# extract the elements into a dictionary using the CSS selector\n",
    "product_data = {\n",
    "    \"Url\": driver.find_element(\n",
    "        By.CSS_SELECTOR, \".woocommerce-LoopProduct-link\"\n",
    "    ).get_attribute(\"href\"),\n",
    "    \"Image\": driver.find_element(By.CSS_SELECTOR, \".product-image\").get_attribute(\n",
    "        \"src\"\n",
    "    ),\n",
    "    \"Name\": driver.find_element(By.CSS_SELECTOR, \".product-name\").text,\n",
    "    \"Price\": driver.find_element(By.CSS_SELECTOR, \".price\").text,\n",
    "}\n",
    "\n",
    "# print the extracted data\n",
    "print(product_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare an empty list to collect the extracted data\n",
    "\n",
    "extracted_products = []\n",
    "\n",
    "# loop through the product containers\n",
    "\n",
    "for product in products:\n",
    "\n",
    "    # extract the elements into a dictionary using the CSS selector\n",
    "    product_data = {\n",
    "        \"Url\": product.find_element(\n",
    "            By.CSS_SELECTOR, \".woocommerce-LoopProduct-link\"\n",
    "        ).get_attribute(\"href\"),\n",
    "        \"Image\": product.find_element(By.CSS_SELECTOR, \".product-image\").get_attribute(\n",
    "            \"src\"\n",
    "        ),\n",
    "        \"Name\": product.find_element(By.CSS_SELECTOR, \".product-name\").text,\n",
    "        \"Price\": product.find_element(By.CSS_SELECTOR, \".price\").text,\n",
    "    }\n",
    "\n",
    "    # append the extracted data to the extracted_product list\n",
    "\n",
    "    extracted_products.append(product_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "data = pd.DataFrame(extracted_products)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **How to Interact With a Web Page as in a Browser**\n",
    "\n",
    "Selenium allows you to mimic human interactions with web pages, enabling actions such as **scrolling**, **clicking**, **hovering**, **filling out forms**, and even **dragging and dropping**. This capability is especially useful when working with dynamic pages or avoiding anti-bot measures.\n",
    "\n",
    "In this section, we'll explore **browser interactions** you might frequently use while scraping with Selenium, focusing on **scrolling** as a vital technique.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:#2E86C1\"><b>Scrolling</b></span>\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>Why Scrolling Matters</b></span>\n",
    "Scrolling is essential when scraping websites that load content dynamically, such as those implementing **infinite scrolling**. These pages use AJAX to fetch more data as you scroll, which requires additional actions to access all the content.\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>Infinite Scrolling Logic</b></span>\n",
    "\n",
    "To scrape such pages, you can simulate continuous scrolling using **JavaScript** within Selenium's `execute_script` method. The process involves:\n",
    "\n",
    "1. **Getting the Initial Page Height**: This serves as a baseline for detecting changes in content.\n",
    "2. **Initiating a Scrolling Action in a Loop**: Use JavaScript to scroll to the bottom of the page.\n",
    "3. **Pausing for Content to Load**: Add a delay using `time.sleep` to allow AJAX calls to complete.\n",
    "4. **Checking for Additional Content**: Compare the updated page height with the initial height to determine if new content has loaded.\n",
    "5. **Breaking the Loop**: Stop when no new content is detected.\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>Key Points to Note</b></span>\n",
    "\n",
    "- **Time Delays**: Adjust the `time.sleep` duration based on the loading speed of the target website. Some sites might require longer pauses for content to appear.\n",
    "- **JavaScript Execution**: Selenium's `execute_script` method is pivotal for simulating browser actions that aren't directly available via the WebDriver API.\n",
    "- **Dynamic Content Handling**: Scrolling is just one step. Ensure your scraper is equipped to handle elements like pop-ups, lazy-loaded images, or CAPTCHA challenges.\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:#28B463\"><b>Benefits of Simulating Scrolling</b></span>\n",
    "\n",
    "- **Access Dynamic Content**: Extract hidden data that only becomes visible after scrolling.\n",
    "- **Mimic User Behavior**: Reduce the likelihood of being flagged as a bot by emulating human-like interactions.\n",
    "- **Handle Large Datasets**: Scrape all content from long pages without manually clicking \"Load More.\"\n",
    "\n",
    "---\n",
    "\n",
    "By mastering scrolling techniques, you'll enhance your ability to scrape dynamic and content-rich websites effectively with Selenium!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "# options.add_argument(\"--headless=new\")\n",
    "\n",
    "driver = webdriver.Chrome(\n",
    "    options=options\n",
    ")\n",
    "\n",
    "driver.get(\"https://www.scrapingcourse.com/infinite-scrolling\")\n",
    "\n",
    "# get the initial scroll height\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "while True:\n",
    "    # scroll to the bottom of the page\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    # wait for more elements to load after scrolling\n",
    "    time.sleep(5)\n",
    "\n",
    "    # get the new scroll height after scrolling\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    # check if new content has loaded\n",
    "    if new_height == last_height:\n",
    "        # if no new content is loaded, break the loop\n",
    "        break\n",
    "\n",
    "    # update the last height\n",
    "    last_height = new_height\n",
    "\n",
    "# extract all product containers\n",
    "products = driver.find_elements(By.CSS_SELECTOR, \".product-item\")\n",
    "\n",
    "# declare an empty list to collect the extracted data\n",
    "extracted_products = []\n",
    "\n",
    "# loop through each product container to extract details\n",
    "for product in products:\n",
    "    product_data = {\n",
    "        \"Name\": product.find_element(By.CSS_SELECTOR, \".product-name\").text,\n",
    "        \"Price\": product.find_element(By.CSS_SELECTOR, \".product-price\").text,\n",
    "    }\n",
    "    extracted_products.append(product_data)\n",
    "\n",
    "# output the data\n",
    "print(extracted_products)\n",
    "\n",
    "# release the resources allocated by Selenium and shut down the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **Handling JavaScript-Rendered Pages in Selenium**\n",
    "\n",
    "Scraping JavaScript-rendered pages requires additional steps since the DOM takes time to fully load. The element you want to scrape may not be immediately available, so you'll need to implement waiting mechanisms.\n",
    "\n",
    "### <span style=\"color:#9B59B6\"><b>Selenium provides three primary ways to handle this</b></span>\n",
    "\n",
    "| **Method**            | **Description**                                                                                      | **Best Use Cases**                                      | **Limitations**                                                                 |\n",
    "|------------------------|------------------------------------------------------------------------------------------------------|--------------------------------------------------------|---------------------------------------------------------------------------------|\n",
    "| **`time.sleep()`**     | Pauses execution for a fixed time interval.                                                         | Handling uncertain wait times or infinite scrolling.   | Inefficient; adds unnecessary delays.                                          |\n",
    "| **`implicitly_wait`**  | Globally waits for elements to be present in the DOM.                                               | Applying a uniform wait for all elements.              | Limited control; doesn’t check visibility or interactability.                  |\n",
    "| **`WebDriverWait`**    | Pauses execution until a specific condition is met or the timeout is reached.                       | Waiting for dynamic elements or specific conditions.    | Requires additional setup and knowledge of Selenium APIs.                      |\n",
    "\n",
    "\n",
    "### <span style=\"color:#F39C12\"><b>Recommendations</b></span>\n",
    "\n",
    "- **Use `WebDriverWait`** for flexibility and efficiency when dealing with dynamic JavaScript-rendered pages.\n",
    "- **Avoid overusing `time.sleep()`**, as it introduces inefficiencies.\n",
    "- **Set a reasonable `implicitly_wait`** time as a fallback for general page loading delays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required libraries\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# instantiate a Chrome options object\n",
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "# set the options to use Chrome in headless mode\n",
    "options.add_argument(\"--headless=new\")\n",
    "\n",
    "# initialize an instance of the Chrome driver (browser) in headless mode\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# visit your target site\n",
    "driver.get(\"https://www.scrapingcourse.com/javascript-rendering\")\n",
    "\n",
    "# wait up to 5 seconds until the image card appears\n",
    "element = WebDriverWait(driver, 5).until(\n",
    "    EC.visibility_of_all_elements_located((By.CSS_SELECTOR, \".product-item\"))\n",
    ")\n",
    "\n",
    "\n",
    "# you are now sure that the product grid has loaded\n",
    "# and can scrape it\n",
    "products = driver.find_elements(By.CSS_SELECTOR, \".product-item\")\n",
    "\n",
    "extracted_products = []\n",
    "\n",
    "for product in products:\n",
    "    product_data = {\n",
    "        \"name\": product.find_element(By.CSS_SELECTOR, \".product-name\").text,\n",
    "        \"price\": product.find_element(By.CSS_SELECTOR, \".product-price\").text,\n",
    "    }\n",
    "\n",
    "    extracted_products.append(product_data)\n",
    "\n",
    "print(extracted_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#D35400\"><b>Popular expected_conditions in Selenium</b></span>\n",
    "\n",
    "Selenium's `expected_conditions` provide flexibility when waiting for specific states or elements during automation. Below is a table of commonly used conditions:\n",
    "\n",
    "| **Condition**                        | **Description**                                                                                   |\n",
    "|--------------------------------------|---------------------------------------------------------------------------------------------------|\n",
    "| **title_contains**                 | Waits until the page title contains a specific string.                                            |\n",
    "| **presence_of_element_located**    | Waits until an HTML element is present in the DOM.                                               |\n",
    "| **visibility_of_element_located**  | Waits until an element already in the DOM becomes visible.                                       |\n",
    "| **text_to_be_present_in_element**  | Waits until a specific text is present in an element.                                            |\n",
    "| **element_to_be_clickable**        | Waits until an HTML element becomes clickable.                                                   |\n",
    "| **alert_is_present**               | Waits until a JavaScript native alert appears.                                                   |\n",
    "| **visibility_of_all_elements_located** | Waits until multiple elements (matching the same selector) become visible.                     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Wait for a Page to Load**\n",
    "\n",
    "**document.readyState:** Execute a script that waits for the document.readyState to complete before interacting further with the DOM. This method involves using the explicit wait method (WebDriverWait) with the expected_conditions to check if the page document and all its resources have finished loading. Again, this method is better since it offers more flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get('https://www.scrapingcourse.com/javascript-rendering')\n",
    "\n",
    "WebDriverWait(driver,10).until(EC.visibility_of_all_elements_located((By.CLASS_NAME,'product-image')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **We can save screenshot of website**\n",
    "\n",
    "this can be useful for capcha handling etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get('https://www.scrapingcourse.com/javascript-rendering')\n",
    "\n",
    "driver.save_screenshot('photo.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also screenshot a specific element. The code below grabs the description section of this demo product page. Note that we've used the ID selector this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = driver.find_element(By.CSS_SELECTOR,'#tab-description')\n",
    "value.screenshot('specific_img.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = driver.find_element(By.CSS_SELECTOR,'.flex-viewport img')\n",
    "value.screenshot('specific_img.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Clicking Specific Button (On a Link)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = driver.find_element(By.CSS_SELECTOR,'.brand-name')\n",
    "value.click()\n",
    "\n",
    "value = driver.find_element(By.CSS_SELECTOR,'.card-page-link')\n",
    "value.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fill Out a Form**\n",
    "\n",
    "Selenium's form-filling feature helps automate actions, such as signing up, logging in, filling out a contact form, or launching a search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get('https://www.scrapingcourse.com/login')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email = driver.find_element(By.CSS_SELECTOR,'#email')\n",
    "password = driver.find_element(By.CSS_SELECTOR,'#password')\n",
    "login = driver.find_element(By.CSS_SELECTOR,'#submit-button')\n",
    "\n",
    "email.send_keys('admin@example.com')\n",
    "password.send_keys('password')\n",
    "login.click()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Execute JavaScript Directly Within the Browser** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selenium provides access to all browser functionalities, including launching JavaScript instructions.\n",
    "\n",
    "The execute_script() method enables you to execute JavaScript instructions synchronously. **That's particularly helpful when the features provided by Selenium aren't enough to achieve your goal.**\n",
    "\n",
    "Let's use **Javascript** to take screenshot of **Description** with better **view.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get(\"https://www.scrapingcourse.com/ecommerce/product/chaz-kangeroo-hoodie/\")\n",
    "\n",
    "card = driver.find_element(By.CSS_SELECTOR,'#tab-description')\n",
    "\n",
    "card_y_location = card.location['y']\n",
    "\n",
    "# \"-100\" to give some extra space and make\n",
    "# ensure the screenshot is taken correctly\n",
    "javaScript = f\"window.scrollBy(0, {card_y_location}-100);\"\n",
    "\n",
    "# execute JavaScript\n",
    "driver.execute_script(javaScript)\n",
    "\n",
    "driver.save_screenshot(\"scrolled-element-screenshot.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Customize Windows Size**\n",
    "\n",
    "Modern sites are responsive and **adapt their layout to the user's screen or browser window size**. Depending on the available space, they may show or hide elements using JavaScript on smaller screens. Selenium allows you to change the browser window's initial size, enabling you to reveal content that might be hidden in the initial viewport. \n",
    "\n",
    "You can achieve this in two ways:\n",
    "-   `options.add_argument(\"--window-size=<width>,<height>\")`.\n",
    "-   `set_window_size(<width>, <height>)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 1: using options.add_argument(\"--window-size=<width>,<height>\")\n",
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "# set the initial window size\n",
    "options.add_argument(\"--window-size=800,600\")\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# print the window size\n",
    "print(driver.get_window_size())  # {\"width\": 800, \"height\": 600}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 2: using set_window_size(<width>, <height>)\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# set the window size\n",
    "driver.set_window_size(1920, 1200)\n",
    "\n",
    "# print the window size\n",
    "print(driver.get_window_size())  # {'width': 1920, 'height': 1200}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Get Around Anti-Scraping Protections With Selenium in Python**\n",
    "\n",
    "You now know how to do web scraping using Selenium in Python. Yet, retrieving data from the web is a challenge, as some sites adopt anti-bot technologies that might detect your scraper as a bot and block it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# run Chrome in headless mode\n",
    "# options = Options()\n",
    "# options.add_argument(\"--headless=new\")\n",
    "\n",
    "# start a driver instance\n",
    "# driver = webdriver.Chrome(options=options)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# open the target website\n",
    "driver.get(\"https://www.g2.com/products/asana/reviews\")\n",
    "\n",
    "# save a screenshot to see what happens\n",
    "# driver.save_screenshot(\"g2-reviews-screenshot.png\")\n",
    "\n",
    "# release the resources allocated by Selenium and shut down the browser\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Change IP Using a Proxy**\n",
    "\n",
    "A proxy service sends requests on your behalf and increases your chances of bypassing IP bans due to rate limiting and geo-restrictions. \n",
    "\n",
    "To see how proxy implementation works in Selenium, grab a [free proxy](https://free-proxy-list.net/) from the Free Proxy List and add it to your scraper, as shown in the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# run Chrome in headless mode\n",
    "options = Options()\n",
    "\n",
    "# set the proxy address\n",
    "proxy_server_ip = \"http://67.43.228.251:3343\"\n",
    "\n",
    "# add the address to Chrome options\n",
    "options.add_argument(f\"--proxy-server={proxy_server_ip}\")\n",
    "\n",
    "# set the options to use Chrome in headless mode\n",
    "# options.add_argument(\"--headless\")\n",
    "\n",
    "# start a driver instance\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# open the target website\n",
    "driver.get(\"https://httpbin.io/ip\")\n",
    "\n",
    "# print your current IP address\n",
    "print(driver.find_element(By.TAG_NAME, \"body\").text)\n",
    "\n",
    "# release the resources allocated by Selenium and shut down the browser\n",
    "# driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
